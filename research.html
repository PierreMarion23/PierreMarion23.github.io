<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Pierre Marion - Research</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="shortcut icon" href="images/favicon.ico" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<img src="images/header/header.svg" alt="" />
				<nav>
					<ul>
						<li><a href="#menu">Menu</a></li>
					</ul>
				</nav>
				</header>

				<!-- Menu -->
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
							<li><a href="index.html">Home</a></li>
							<li><a href="research.html">Research</a></li>
							<li><a href="teaching.html">Teaching</a></li>
							<li><a href="software.html">Software and projects</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Research</h1>
							
							<section>
								<h2>Interests</h2>

								<p>My current research interests regard the theory of deep learning. During my PhD, I studied in particular the connection between large-depth residual networks and neural differential equations. You're welcome to download my <a href="files/thesis.pdf" target="_blank">PhD thesis</a>.
								I worked previously on Natural Language Processing (following an internship at Google) and on Quasi-Monte Carlo methods (following an internship at University of Montreal).</p>

								<h2>Publications</h2>
								<h3>Peer-reviewed publications</h3>
									<ul>
										<li>P. Marion, L. Chizat, <i>Deep linear networks for regression are implicitly regularized towards flat minima</i>, accepted for publication at NeurIPS 2024. <a href="https://arxiv.org/abs/2405.13456" target="_blank">Online access</a></li>
										<li>P. Marion*, Y.-H. Wu*, M. Sander, G. Biau, <i>Implicit regularization of deep residual networks towards neural ODEs </i>, ICLR 2024, <i>spotlight presentation</i>, May 2024. <a href="https://arxiv.org/abs/2309.01213" target="_blank">Online access</a></li>
										<li>P. Marion, <i>Generalization bounds for neural ordinary differential equations and deep residual networks</i>, Advances in Neural Information Processing Systems 36 (NeurIPS 2023), December 2023. <a href="https://arxiv.org/abs/2305.06648" target="_blank">Online access</a></li>
										<li>P. Marion, R. Berthier, <i>Leveraging the two-timescale regime to demonstrate convergence of neural networks</i>, Advances in Neural Information Processing Systems 36 (NeurIPS 2023), December 2023. <a href="https://arxiv.org/abs/2304.09576" target="_blank">Online access</a></li>
										<li>A. Fermanian*, P. Marion*, J.P. Vert, G. Biau, <i>Framing RNN as a kernel method: A neural ODE approach</i>, Advances in Neural Information Processing Systems 34 (NeurIPS 2021), <i>oral presentation</i>, December 2021. <a href="https://proceedings.neurips.cc/paper/2021/hash/18a9042b3fc5b02fe3d57fea87d6992f-Abstract.html" target="_blank">Online access</a></li>
										<li>P. Marion, P. Nowak, F. Piccinno, <i>Structured Context and High-Coverage Grammar for Conversational Question Answering over Knowledge Graphs</i>, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021), November 2021. <a href="https://aclanthology.org/2021.emnlp-main.695/" target="_blank">Online access</a> </li>
										<li>P. L’Ecuyer, P. Marion, M. Godin, and F. Puchhammer, <i>A Tool for Custom Construction of QMC and RQMC Point Sets</i>, Proceedings of Monte Carlo and Quasi-Monte Carlo Methods 2020, August 2020. <a href="https://www.springerprofessional.de/en/a-tool-for-custom-construction-of-qmc-and-rqmc-point-sets/22166480" target="_blank">Online access.</a></li>
										<li>P. Marion, M. Godin, and P. L’Ecuyer, <i>An algorithm to compute the t-value of a digital net
										and of its projections</i>, Journal of Computational and Applied Mathematics, June 2020. <a href="https://doi.org/10.1016/j.cam.2019.112669" target="_blank">Online access</a> </li>
									</ul>

									<p>(stars denote first co-authors.)</p>
								<h3>Preprints</h3>
									<ul>
										<li>P. Marion, R. Berthier, G. Biau, C. Boyer, <i>Attention layers provably solve single-location regression</i>, arXiv:2410.01537, October 2024. <a href="https://arxiv.org/abs/2410.01537" target="_blank">Online access</a></li>
										<li>P. Marion, A. Korba, P. Bartlett, M. Blondel, V. De Bortoli, A. Doucet, F. Llinares-López, C. Paquette, Q. Berthet, <i>Implicit Diffusion: Efficient Optimization through Stochastic Sampling</i>, arXiv:2402.05468, February 2024. <a href="https://arxiv.org/abs/2402.05468" target="_blank">Online access</a></li>
										<li>P. Marion, A.Fermanian, G. Biau, J.P. Vert, <i>Scaling ResNets in the Large-depth Regime</i>, arXiv:2206.06929, June 2022. <a href="https://arxiv.org/abs/2206.06929" target="_blank">Online access</a></li>
									</ul>

								
								<h3>Other publications</h3>
									<ul>
										<li>C. Lucas, P. Marion, <i>Recherche et innovation : comment rapprocher sphère publique et privée</i>, Les Docs de La Fabrique, Paris, Presses des Mines, 2022.</li>
										<li>P. Marion, <i>Comment compter nos morts du Covid ?</i>, The Conversation, May 2020, <a href="https://theconversation.com/comment-compter-nos-morts-du-covid-138278" target="_blank">link</a> (in French)</li>
										<li>O. Borderies, O. Coudray and P. Marion,
											<i>Authoring Custom Jupyter Widgets</i>, The Jupyter Blog, March 2018, <a href="https://blog.jupyter.org/authoring-custom-jupyter-widgets-2884a462e724" target="_blank">link</a></li>	
									</ul>

								<h2>Talks and posters</h2>
									<ul>
										<li><i> Three stories on deep linear networks</i>, SIGMA 2024 Workshop, CIRM, Marseille, France, October 2024</li>
										<li><i> Three stories on deep linear networks</i>, Séminaire du département DATA, Laboratoire LJK, Grenoble, France, September 2024</li>
										<li><i> Three stories on deep linear networks</i>, Journées MAS 2024, Poitiers, France, August 2024</li>
										<li><i>Implicit Diffusion: Efficient Optimization through Stochastic Sampling</i>, FGS conference on optimization, Gijon, Spain, June 2024</li>
										<li><i> Three stories on deep linear networks</i>, Universitat Pompeu Fabra, Barcelona, Spain, June 2024</li>
										<li><i> Three stories on deep linear networks</i>, séminaire de l'équipe Inria Sierra, Paris, France, June 2024</li>
										<li><i> Régularisation implicite des réseaux de neurones profonds vers des EDO neuronales</i>, Journées de Statistique, Bordeaux, France, May 2024</li>
										<li><i>Modèles de diffusion pour la génération d'images : algorithmes et (un peu de) théorie</i>, Geolearning-RESSTE day, Paris, France, May 2024</li>
										<li><i>	Implicit regularization of deep residual networks towards neural ODEs, poster at ICLR 2024, Vienna, Austria, May 2024</li>
										<li><i>Modèles de diffusion pour la génération d'images : algorithmes et (un peu de) théorie</i>, ML Meetup, Nantes, France, April 2024</li>
										<li><i>Implicit Diffusion: Efficient Optimization through Stochastic Sampling</i>, Mostly Monte Carlo Seminar, Paris, France, March 2024</li>
										<li><i>Implicit Diffusion: Efficient Optimization through Stochastic Sampling</i>, Google, Paris, France, February 2024</li>
										<li><i>Generalization bounds for neural ordinary differential equations and deep residual networks</i>, NeurIPS@Paris 2023, Paris, France, December 2023</li>
										<li><i>Leveraging the two-timescale regime to demonstrate convergence of neural networks</i>, NeurIPS@Paris 2023, Paris, France, December 2023</li>
										<li><i>	Framing RNNs as a kernel method through the signature </i>, Rencontre sur les signatures, applications et machine learning, Pau, France, December 2023</li>
										<li><i>	Neural ODEs and residual neural networks </i>, séminaire de l'équipe Inria HeKA, Paris, France, September 2023</li>
										<li><i>	Generalization bounds for neural ordinary differential equations and deep residual networks </i>, poster at StatMathAppli 2023, Fréjus, France, September 2023</li>
										<li><i>Apprentissage par optimisation stochastique bi-échelle pour les réseaux de neurones</i>, Journées de Statistique, Bruxelles, Belgium, July 2023</li>
										<li><i>Leveraging the two-timescale regime to demonstrate convergence of neural networks</i>, poster at FoCM 2023, Paris, France, June 2023</li>
										<li><i>Neural network training with stochastic gradient descent in a two-timescale regime</i>, PhD students seminar at LPSM, Sorbonne Université, Paris, France, March 2023</li>
										<li><i>Framing RNN as a kernel method: a neural ODE approach</i>, ICSDS 2022, Florence, Italy, December 2022</li>
										<li><i>Recurrent neural networks are kernel methods: a neural ODE approach</i>, EPFL, Switzerland, December 2022</li>
										<li><i>Signatures and continuous-time neural networks for sequential data</i>, séminaire de l'équipe Inria MIND, Saclay, France, November 2022</li>
										<li><i>Limites profondes des réseaux de neurones</i>, MathInnov Day, Paris, France, October 2022</li>
										<li><i>Scaling ResNets in the Large-depth Regime</i>, conférence LOL 2022, Marseille, France, October 2022</li>
										<li><i>Framing RNN as a kernel method: a neural ODE approach</i>, Summer Cluster on deep learning theory, Simons Institute, Berkeley, USA, July 2022</li>
										<li><i>Scaling ResNets in the Large-depth Regime</i>, Journées de Statistique, Lyon, France, June 2022</li>
										<li><i>Towards understanding residual neural networks via the large-depth limit</i>, Journée Équation d'évolution et Machine Learning, Paris, France, May 2022</li>
										<li><i>Large-depth limit for residual neural networks</i>, Rencontre des Jeunes Statisticiens, Porquerolles, France, April 2022</li>
										<li><i>Framing RNN as a kernel method: A neural ODE approach</i>, Séminaire Parisien de Statistique, Paris, France, January 2022</li>
										<li><i>Framing RNN as a kernel method: A neural ODE approach</i>, NeurIPS@Paris 2021, Paris, France, December 2021</li>
										<li><i>Framing RNN as a kernel method: A neural ODE approach</i>, oral at NeurIPS 2021 (online), December 2021 (<a href="https://www.youtube.com/watch?v=2_MF2LX9Q5E&ab_channel=ArtificialIntelligence" target="_blank">Youtube video</a>)</li>
										<li><i>Framing RNN as a kernel method: A neural ODE approach</i>, PhD students seminar at LPSM, Sorbonne Université, Paris, France, November 2021</li>
										<li><i>Framing RNN as a kernel method: A neural ODE approach</i>, DeepMath Working Group, Sorbonne Université, Paris, France, November 2021</li>
										<li><i>A primer on Neural ODEs</i>, Institut Mathématique de Toulouse, France, Septembre 2021</li>
										<li><i>"Who plays Gandalf in LOTR?" - Natural Language Processing on structured data</i>, PhD students seminar at LPSM, Sorbonne Université, Paris, France, May 2021</li>
										<li><i>Algorithms and Software for Custom Digital Net Constructions</i>, <a href="https://mcqmc20.web.ox.ac.uk/home">MCQMC 2020</a>, online, August 2020 (<a href="https://www.youtube.com/watch?v=GDSoxFeCT4A" target="_blank">Youtube video</a>) </li>
										<li><i>How to beat randomness? A Quasi-Monte Carlo methods overview</i>, PROWLER.io, Cambridge, UK, November 2019</li>
										<li><i>Deep learning beyond the hype</i>, Corps des Mines, November 2019</li>
									</ul>

								<h2>Other activities</h2>
									<ul>
										<li>2024-2025:</li>
										<ul>
											<li>Organizer of NeurIPS@Paris 2024, a 200-people meetup before NeurIPS 2024</li>
										</ul>
										<li>2022-2023:</li>
										<ul>
											<li>Organizer of NeurIPS@Paris 2022, a 200-people meetup before NeurIPS 2022</li>
											<li>Organizer of the session "Differential equations and machine learning" at ICSDS 2022</li>
											<li>Organizer of the PhD seminar at LPSM</li>
											<li>Organizer of the PhD seminar at Corps des Mines</li>
										</ul>
										<li>2021-2022:</li>
										<ul>
											<li>Organizer of NeurIPS@Paris 2021, a 150-people meetup during NeurIPS 2021</li>
											<!-- <li>Organizer of a machine learning reading group at Sorbonne University</li> -->
											<li>Organizer of the PhD seminar at Corps des Mines</li>
										</ul>
										<li>2020-2021:</li>
										<ul>
											<li>Organizer of the research seminar at Corps des Mines</li>
										</ul>
										<li>Reviewer for journals (JASA, Annals of Statistics, Information and Inference, JMLR, SIMODS, Bernoulli) and conferences (NeurIPS 2023, ICLR 2024, NeurIPS 2024, ICLR 2025, AISTATS 2025). Top reviewer at NeurIPS 2023.</li>
									</ul>
		
							</section>
								
						
						
						</div>
					</div>

				<!-- Footer -->
				<footer id="footer">
					<div class="inner">
						<section>
							<h2>Let's get in touch!</h2>
							<ul class="icons">
								<li><a href="mailto:pierre.marion@epfl.ch" class="icon solid style2 fa-envelope"><span class="label">E-Mail</span></a></li>
								<li><a href="https://www.linkedin.com/in/pierre-marion-816474130/" class="icon brands style2 fa-linkedin"><span class="label">LinkedIn</span></a></li>			
								<li><a href="https://github.com/PierreMarion23" class="icon brands style2 fa-github"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
						<ul class="copyright">
							<li>&copy; Pierre Marion. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</div>
				</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>